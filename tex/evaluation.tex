% !TEX root =  main.tex
\section{Knowledge Base of Processes}

We propose to generate process knowledge for the target domain of grade-level science ranging from levels four through twelve.
Using the domain texts, glossaries, and exam questions, we will manually identify the processes of interest.
We anticipate to build a list of around 2000 processes.  We propose both an intrinsic evaluation that measures the accuracy of the extracted roles, 
and an external evaluation where we assess the utility of the roles in question answering.

\subsection{Intrinsic Evaluation}

Our goal is to acquire knowledge about processes along relevant role dimensions.
Following our prior work on schema evaluation~\cite{balasubramanian2013generating}, we propose a crowdsourcing approach for evaluating the generated knowledge. We will devise an evaluation methodology that assesses the precision and recall of relevant information about the process. 

\subsection{External Evaluation}

We propose to closely collaborate with the Allen Institute for Artificial Intelligence (AI2) for evaluating the knowledge\footnote{Please see attached letter of support.} for use in reasoning-based questions in grade-level science exams. As part of preliminary work we have built a question answering system that leverages semantic roles to recognize process questions~\cite{louvan2015:kcap}. We will extend this approach to work with the substantially larger knowledge base of semantic roles generated by this work.

We propose to work with AI2 to leverage their large collections of question banks. 
We will also work with them to author new questions if need be.
Creating these questions is a difficult task for non-experts without careful guidance.
We will also work with the graduate students aiming for teaching professions to collect questions and question templates to identify the kinds of understanding that needs to be tested. We can then scale out the question acquisition process via crowd sourcing. 


