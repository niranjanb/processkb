% !TEX root =  main.tex
\section{Background and Preliminary Work}

\subsection{Background and Prior Work}
%Many AI tasks such as question answering require ability to interpret and reason about scenarios and situations.

Knowledge extraction and reasoning are central for many Artificial Intelligence systems. 
Recently introduced tasks such as {\em MCTest} reading comprehension challenge~\cite{richardson2013mctest}, grade-level science exams~\cite{clark2015elementary}, and process comprehension tasks~\cite{berantSrikumar14} serve as excellent benchmarks for developing reasoning-based question answering systems. 
These tasks test the ability of the systems to interpret and reason about scenarios and situations.
Knowledge requirements analysis for grade-level science exams shows the need for deep inference supporting knowledge~\cite{chb2013:akbc,clark2014:akbc}.
Also, even with advanced state-of-the-art reasoning techniques, shallow text-derived knowledge is ineffective due to lexical and syntactic variations in language~\cite{khot2015:emlnlp}. Scalable acquisition of deeper semantic knowledge is essential for effective reasoning in these tasks. 

Much progress has been made on question answering involving simple facts~\cite{berant2013semantic,fader2014open,bordes2014open,reddy2014large}. 
This is in large part due to the availability of large scale curated relational knowledge bases such as Freebase, coupled with significant advances in automatic relation extraction~\cite{schmitz2012open,carlson2010toward,suchanek2007yago}. Similar advances in large scale inference-supporting knowledge is vital for making significant progress in reasoning-based QA tasks.

In this work, we focus on knowledge acquisition methods with grade science exams as an end application. 
In particular we focus on a subset of knowledge about physical, chemical, and other natural phenomena.
The goal is to derive knowledge that allows effective reasoning about scenarios involving these phenomena. We refer to this as process knowledge. 
\footnote{Our representation and acquisition methods will be targeted towards the grade science benchmarks but the methodology is general and can be applied to other tasks and domains.}



\todo{Needs a rewrite. Don't read yet.}
Much of SRL work has focused on interpreting sentences i.e., identifying roles expressed in a sentence.
Our goal is different. 
We seek to acquire knowledge about processes and represent them in terms of semantic roles.
SRL is a challenging structure prediction task which often requires 

Prior work either resorted to supervised learning techniques or to semi-supervised approaches 
However, obtaining training data is often difficult and laborious, especially for complex tasks such as SRL.
Several prior approaches have used semi-supervised learning (SSL) approaches to address this issue. 

We propose a different approach, where we explicitly find sentences that express the roles of interest and do a joint inference of role labels across all sentences. 
This allows us greater flexibility in choosing which sentences to use (we can maximize role coverage, diversity of instances etc.).
Aligning the various spans within these sentences allows us to do collective labeling (similar in spirit to transduction learning).


Our central premise is that the entities involved in a process and the roles they play provide a powerful representation for reasoning and QA. 
Similar representations have been shown to be useful for Open-domain factoid question answering~\cite{shen2007using,pizzato2008indexing}, 
and reading comprehension tasks on process descriptions~\cite{berantSrikumar14}.


\subsection{Preliminary Work}

As preliminary work, we first analyzed the knowledge requirements for a set of questions targeting around 150 processes. 
While a small collection of general purpose roles (e.g., Undergoer, Result, Enabler, Trigger) capture the key semantic elements for a 
majority of the processes~\cite{louvan2015:kcap}, we also find that a set of domain specific roles (e.g., Direction, Medium, Physical\_Property, Chemical\_Property) are also critical. We propose to compose a representation that combines both general and domain specific roles. 

Note that these roles are specified based on a canonical view of the process. One of the key challenges in question answering is in overcoming the variability of expression. Labeling with respect to a canonicalized view of the process is a deliberate attempt at addressing the variability problem. 

Manually specifying roles for each process is not desirable from a scalability standpoint.
%We will investigate scalable methods for automatically identifying the appropriate roles for a process. 
Lexical and syntactic cues are indicative of certain roles in sentences. 
For example a {\em [process] happens by [event]} pattern suggests the presence of a manner role, and the presence of locative prepositions can indicate location or orientation roles. 
%Note that the goal of this step is to simply assess whether a particular role applies to the process frame and not to accurately determine the role filler itself.~\footnote{
%Many cues are frame specific and using them in general for all frames can be problematic. 
%However, the goal here is to be inclusive and determine which roles apply. 
%It may be possible to over generate the frame with many plausible roles in this stage and prune it down in the next step if finding role fillers fails.} 
%Furthermore, by aggregating these cues over large number of sentences, we can make reliable assessments about the importance of certain roles.~\footnote{
%Some roles might be left implicit as common sense knowledge. 
%Those remain beyond the scope of this work.} 
